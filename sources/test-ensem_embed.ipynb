{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc, itertools\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, accuracy_score,\\\n",
    "f1_score, precision_score, recall_score, roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set options\n",
    "embed_ver = [\"clstm\", \"esm2\", \"bert\", \"t5\"]\n",
    "data_path = \"../data/test_exam\"\n",
    "model_path = \"../models/classifier_ensem/\"\n",
    "result_path = \"../result/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_num = 3\n",
    "unit_decrease = 2\n",
    "batch_size = 256\n",
    "col_str = ['file_id', 'organism', 'locus_tag', 'ess']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data list for test dataset\n",
    "ts_data = {\n",
    "    \"data1\": [\"C018\"],  # \"Escherichia coli K-12 BW25113\"\n",
    "    \"data2\": [\"C016\"],  # \"Escherichia coli K-12 MG1655\"\n",
    "    \"data3\": [\"O046\"],  # \"synthetic bacterium JCVI-Syn3A\"\n",
    "    \"data4\": [\"C048\"],  # Bacteroides thetaiotaomicron VPI-5482\n",
    "    \"data5\": [\"C050\"]  # Salmonella enterica subsp. enterica serovar Typhimurium str. 14028S\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to record perfomance result\n",
    "def record_perform(comb_ver, file_id, organ, y_real, y_conf, y_prd):\n",
    "    y_real = y_real.cpu().numpy()\n",
    "    y_conf = y_conf.cpu().numpy()\n",
    "    y_prd = y_prd.cpu().numpy()\n",
    "    \n",
    "    if file_id != \"O046\":\n",
    "        auc_roc = [roc_auc_score(y_real, y_conf)]\n",
    "        auc_pr = [average_precision_score(y_real, y_conf)]\n",
    "    else:\n",
    "        auc_roc = None\n",
    "        auc_pr = None\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_real, y_prd).ravel()\n",
    "    \n",
    "    result = pd.DataFrame({\n",
    "        \"comb\": [comb_ver],\n",
    "        \"file\": [file_id],\n",
    "        \"organism\": [organ],\n",
    "        \"tp\": [tp],\n",
    "        \"fp\": [fp],\n",
    "        \"tn\": [tn],\n",
    "        \"fn\": [fn],\n",
    "        \"mcc\": [matthews_corrcoef(y_real, y_prd)],\n",
    "        \"acc\": [accuracy_score(y_real, y_prd)],\n",
    "        \"f1\": [f1_score(y_real, y_prd)],\n",
    "        \"prc\": [precision_score(y_real, y_prd)],\n",
    "        \"rec\": [recall_score(y_real, y_prd)],\n",
    "        \"npv\": [precision_score(1 - y_real, 1 - y_prd)],\n",
    "        \"tnr\": [recall_score(1 - y_real, 1 - y_prd)],\n",
    "        \"auc-roc\": auc_roc,\n",
    "        \"auc-pr\": auc_pr\n",
    "    })\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model architecture\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, unit_decrease):\n",
    "        super(Classifier, self).__init__()\n",
    "        layers = [nn.BatchNorm1d(input_size), nn.Dropout(0.5)]\n",
    "        in_dim = input_size\n",
    "        out_dim = 1024\n",
    "        for i in range(num_layers):            \n",
    "            out_dim = max(2, out_dim // unit_decrease)\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            self.initialize_weights(layers[-1])\n",
    "            layers.append(nn.GELU())\n",
    "            in_dim = out_dim\n",
    "        layers.append(nn.Linear(out_dim, 1))\n",
    "        self.cls_block = nn.Sequential(*layers)\n",
    "        \n",
    "    def initialize_weights(self, layer):\n",
    "        nn.init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='linear')\n",
    "        if layer.bias is not None:\n",
    "            nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.cls_block(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [(e_ver, pd.read_csv(data_path + f\"data_emb-{e_ver}.csv\")) for e_ver in embed_ver]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.DataFrame()\n",
    "\n",
    "for r in range(2, len(dfs) + 1):\n",
    "    combs = list(itertools.combinations(dfs, r))\n",
    "    \n",
    "    for comb in combs:\n",
    "        comb_ver = \"_\".join([df[0] for df in comb])\n",
    "        print(f\"\\n>>>> {comb_ver} <<<<\")\n",
    "        \n",
    "        # merge dataset\n",
    "        data = comb[0][1]\n",
    "        for df in comb[1:]:\n",
    "            data = pd.merge(data, df[1], on=col_str, suffixes=(\"\", f\"_{df[0]}\"))\n",
    "        \n",
    "        display(\"Raw data:\", data)\n",
    "        \n",
    "        # calculate mean of confidences\n",
    "        col_num = [col for col in data.columns if col not in col_str]\n",
    "    \n",
    "        # get test datasets\n",
    "        loc_ts = {}\n",
    "        data_ts = {}\n",
    "        org_ts = {}\n",
    "        for ts_ver, ids in ts_data.items():\n",
    "            # get test sample locations\n",
    "            loc_ts[ts_ver] = data['file_id'].isin(ids)\n",
    "            # get test samples\n",
    "            data_ts[ts_ver] = data[loc_ts[ts_ver]]\n",
    "            org = []\n",
    "            # get test organism list\n",
    "            for i in ids:\n",
    "                organ = data_ts[ts_ver]['organism'][data_ts[ts_ver]['file_id'] == i].to_list()\n",
    "                if len(organ) > 0:\n",
    "                    org.append(organ[0])\n",
    "            org_ts[ts_ver] = org    \n",
    "            print(\"Test dataset(\" + ts_ver + \"):\", data_ts[ts_ver].shape)\n",
    "        print(\"Test organism:\", org_ts, len(org_ts))\n",
    "        \n",
    "        # split info.& inputs & labels of the test datasets\n",
    "        info_ts = {}\n",
    "        y_ts = {}\n",
    "        test_loader = {}\n",
    "        for ts_ver, df in data_ts.items():\n",
    "            info_ts[ts_ver] = df[col_str]\n",
    "            X_ts = torch.tensor(df.iloc[:, len(col_str):].astype('float32').values)\n",
    "            y_ts[ts_ver] = torch.tensor(df['ess'].astype('float32').values)\n",
    "            print(\"Splited test dataset(\" + ts_ver + \"):\", X_ts.shape, y_ts[ts_ver].shape)                    \n",
    "            # generate dataloader by the test datasets\n",
    "            dataset_ts = TensorDataset(X_ts, y_ts[ts_ver])\n",
    "            test_loader[ts_ver] = DataLoader(dataset_ts, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        ## Test model ##\n",
    "        # set model name\n",
    "        model_name = f\"cls-{comb_ver}\"\n",
    "        print(f\"\\n===== Test model: {model_name} ====\")\n",
    "        # generate model instance\n",
    "        model = Classifier(\n",
    "            input_size=X_ts.shape[-1],\n",
    "            num_layers=layer_num,\n",
    "            unit_decrease=unit_decrease\n",
    "        ).to(device)\n",
    "\n",
    "        # load model weight\n",
    "        model.load_state_dict(torch.load(model_path + model_name + \".pt\", map_location=device))\n",
    "        model.eval()\n",
    "        \n",
    "        # model evaluations by test dataset\n",
    "        df_pred = pd.DataFrame()\n",
    "        total_result = {key: [] for key in col_str + ['logit', 'conf']}\n",
    "\n",
    "        for ts_ver, ids in ts_data.items():\n",
    "            results = {key: [] for key in total_result.keys()}\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in test_loader[ts_ver]:\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    y_batch = y_batch.to(device)\n",
    "                    # prediction\n",
    "                    preds = model(X_batch).squeeze()\n",
    "                    # gather the result\n",
    "                    results['logit'].extend(preds.cpu().tolist())\n",
    "            # gather testset info.\n",
    "            for key in col_str:\n",
    "                results[key].extend(info_ts[ts_ver][key].tolist())\n",
    "            \n",
    "            gc.collect()\n",
    "\n",
    "            # convert logits to confidences & classes\n",
    "            prd_conf = torch.sigmoid(results['logit'])\n",
    "            prd_cls = (prd_conf >= 0.5).int()\n",
    "\n",
    "            # gather result of the predicted essentiality\n",
    "            for key, val in results.items():\n",
    "                total_result[key].extend(val)\n",
    "            pred_ts = pd.DataFrame({key: results[key] for key in col_str + ['conf']})\n",
    "            df_pred = pd.concat([df_pred, pred_ts], ignore_index=True)\n",
    "            \n",
    "            # get evaluation row by testset\n",
    "            eval_ts = record_perform(\n",
    "                comb_ver=f\"{comb_ver}\",\n",
    "                test_ver=ts_ver,\n",
    "                file_id=\"+\".join(ids),\n",
    "                organ=\"+\".join(org_ts[ts_ver]),\n",
    "                y_real=results['ess'],\n",
    "                y_conf=prd_conf,\n",
    "                y_prd=prd_cls,\n",
    "            )\n",
    "            df_eval = pd.concat([df_eval, eval_ts], ignore_index=True)\n",
    "            print(f\"- Test in {ts_ver} was done.\")\n",
    "\n",
    "        # save the model prediction result\n",
    "        df_pred.to_csv(f\"{result_path}prd-embed_ensem/{model_name}.csv\", index=False)\n",
    "\n",
    "        # convert logits to confidences & classes\n",
    "        prd_conf = torch.sigmoid(total_result['logit'])\n",
    "        prd_cls = (prd_conf >= 0.5).int()\n",
    "\n",
    "        # get total mean row\n",
    "        eval_ts = record_perform(\n",
    "            comb_ver=f\"{comb_ver}\",\n",
    "            test_ver=\"test_all\",\n",
    "            file_id=\"total\",\n",
    "            organ=\"all\",\n",
    "            y_real=total_result['ess'],\n",
    "            y_conf=prd_conf,\n",
    "            y_prd=prd_cls\n",
    "        )\n",
    "        df_eval = pd.concat([df_eval, eval_ts], ignore_index=True)\n",
    "\n",
    "# save the model perfomance result\n",
    "df_eval.to_csv(f\"{result_path}eval-embed_ensem.csv\", index=False)\n",
    "display(\"Model performance:\", df_eval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
