{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, itertools, gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, accuracy_score,\\\n",
    "f1_score, precision_score, recall_score, roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set options\n",
    "embed_ver = [\"clstm\", \"esm2\", \"bert\", \"t5\"]\n",
    "result_path = \"../result/prd-indiv_class/\"\n",
    "save_path = \"../result/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_str = ['file_id', 'organism', 'locus_tag', 'ess']\n",
    "layer_num = 3\n",
    "unit_decrease = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data list for test dataset\n",
    "ts_data = {\n",
    "    \"data1\": [\"C018\"],  # \"Escherichia coli K-12 BW25113\"\n",
    "    \"data2\": [\"C016\"],  # \"Escherichia coli K-12 MG1655\"\n",
    "    \"data3\": [\"O046\"],  # \"synthetic bacterium JCVI-Syn3A\"\n",
    "    \"data4\": [\"C048\"],  # Bacteroides thetaiotaomicron VPI-5482\n",
    "    \"data5\": [\"C050\"]  # Salmonella enterica subsp. enterica serovar Typhimurium str. 14028S\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to record perfomance result\n",
    "def record_perform(comb_ver, file_id, organ, y_real, y_conf, y_prd):    \n",
    "    if file_id != \"O046\":\n",
    "        auc_roc = [roc_auc_score(y_real, y_conf)]\n",
    "        auc_pr = [average_precision_score(y_real, y_conf)]\n",
    "    else:\n",
    "        auc_roc = None\n",
    "        auc_pr = None\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_real, y_prd).ravel()\n",
    "    \n",
    "    result = pd.DataFrame({\n",
    "        \"comb\": [comb_ver],\n",
    "        \"file\": [file_id],\n",
    "        \"organism\": [organ],\n",
    "        \"tp\": [tp],\n",
    "        \"fp\": [fp],\n",
    "        \"tn\": [tn],\n",
    "        \"fn\": [fn],\n",
    "        \"mcc\": [matthews_corrcoef(y_real, y_prd)],\n",
    "        \"acc\": [accuracy_score(y_real, y_prd)],\n",
    "        \"f1\": [f1_score(y_real, y_prd)],\n",
    "        \"prc\": [precision_score(y_real, y_prd)],\n",
    "        \"rec\": [recall_score(y_real, y_prd)],\n",
    "        \"npv\": [precision_score(1 - y_real, 1 - y_prd)],\n",
    "        \"tnr\": [recall_score(1 - y_real, 1 - y_prd)],\n",
    "        \"auc-roc\": auc_roc,\n",
    "        \"auc-pr\": auc_pr\n",
    "    })\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [(e_ver, pd.read_csv(result_path + f\"cls-{e_ver}.csv\")) for e_ver in embed_ver]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_eval = pd.DataFrame()\n",
    "\n",
    "for r in range(2, len(dfs) + 1):\n",
    "    combs = list(itertools.combinations(dfs, r))\n",
    "    \n",
    "    for comb in combs:\n",
    "        comb_ver = \"_\".join([df[0] for df in comb])\n",
    "        print(f\"\\n>>>> {comb_ver} <<<<\")\n",
    "        \n",
    "        # merge dataset\n",
    "        data = comb[0][1]\n",
    "        for df in comb[1:]:\n",
    "            data = pd.merge(data, df[1], on=col_str, suffixes=(\"\", f\"_{df[0]}\"))\n",
    "        \n",
    "        display(\"Raw data:\", data)\n",
    "        \n",
    "        # calculate mean of confidences\n",
    "        col_num = [col for col in data.columns if col not in col_str]\n",
    "        data['conf_mean'] = data[col_num].mean(axis=1)\n",
    "    \n",
    "        # get test datasets\n",
    "        loc_ts = {}\n",
    "        data_ts = {}\n",
    "        org_ts = {}\n",
    "        for ts_ver, ids in ts_data.items():\n",
    "            # get test sample locations\n",
    "            loc_ts[ts_ver] = data['file_id'].isin(ids)\n",
    "            # get test samples\n",
    "            data_ts[ts_ver] = data[loc_ts[ts_ver]]\n",
    "            org = []\n",
    "            # get test organism list\n",
    "            for i in ids:\n",
    "                organ = data_ts[ts_ver]['organism'][data_ts[ts_ver]['file_id'] == i].to_list()\n",
    "                if len(organ) > 0:\n",
    "                    org.append(organ[0])\n",
    "            org_ts[ts_ver] = org\n",
    "    \n",
    "            print(\"Test dataset(\" + ts_ver + \"):\", data_ts[ts_ver].shape)\n",
    "        print(\"Test organism:\", org_ts, len(org_ts))\n",
    "        \n",
    "        # get the total test dataset info.\n",
    "        loc_ts_all = [sum(loc) >= 1 for loc in zip(*loc_ts.values())]\n",
    "        info_ts_all = data.loc[loc_ts_all, col_str]\n",
    "        \n",
    "        ## evaluations by test dataset ##\n",
    "        for ts_ver, ids in ts_data.items():\n",
    "            prd_conf = data_ts[ts_ver]['conf_mean']\n",
    "            prd_cls = (prd_conf >= 0.5).astype(int)\n",
    "            # performances by testset\n",
    "            perform = record_perform(\n",
    "                comb_ver=comb_ver,\n",
    "                file_id=\"+\".join(ids),\n",
    "                organ=\"+\".join(org_ts[ts_ver]),\n",
    "                y_real=data_ts[ts_ver]['ess'],\n",
    "                y_conf=prd_conf,\n",
    "                y_prd=prd_cls,\n",
    "            )\n",
    "            df_eval = pd.concat([df_eval, perform], ignore_index=True)\n",
    "            print(f\"- Test in {ts_ver} was done.\")\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        # evaluation for total test dataset\n",
    "        prd_conf = data.loc[loc_ts_all, 'conf_mean']\n",
    "        prd_cls = (prd_conf >= 0.5).astype(int)\n",
    "\n",
    "        # performances on total testset\n",
    "        perform = record_perform(\n",
    "            comb_ver=comb_ver,\n",
    "            test_ver=\"test_all\",\n",
    "            file_id=\"total\",\n",
    "            organ=\"all\",\n",
    "            y_real=data.loc[loc_ts_all, 'ess'],\n",
    "            y_conf=prd_conf,\n",
    "            y_prd=prd_cls\n",
    "        )\n",
    "        df_eval = pd.concat([df_eval, perform], ignore_index=True)\n",
    "        print(f\"- Test in total testset was done.\")\n",
    "        \n",
    "        # concatenate the protein info. & predicted confidences\n",
    "        df_prd = pd.DataFrame(prd_conf, columns=[\"conf_mean\"], index=info_ts_all.index)\n",
    "        df_prd = pd.concat([info_ts_all, df_prd], axis=1)\n",
    "\n",
    "        # save the model prediction result\n",
    "        df_prd.to_csv(f\"{result_path}prd-simple_ensem/{comb_ver}.csv\", index=False)\n",
    "    \n",
    "        print(f\"- Prediction by {comb_ver} was done.\")\n",
    "\n",
    "# save the model perfomance result\n",
    "display(\"Model performance:\", df_eval)\n",
    "df_eval.to_csv(f\"{result_path}eval-simple_ensem.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
